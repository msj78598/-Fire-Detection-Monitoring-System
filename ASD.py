import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import av
import torch
import cv2
import numpy as np
from PIL import Image
import time

# âœ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ YOLOv5
MODEL_PATH = "best.pt"

# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
try:
    model = torch.hub.load('ultralytics/yolov5', 'custom', path=MODEL_PATH, source="github")
    model.conf = 0.1  # Ø¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø© Ø¥Ù„Ù‰ 0.1
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ YOLOv5 Ø¨Ù†Ø¬Ø§Ø­!")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ YOLOv5: {e}")

# âœ… ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="Fire Detection Monitoring", page_icon="ğŸ”¥", layout="wide")

# âœ… ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.title("ğŸ”¥ Fire Detection Monitoring System")
st.markdown("<h4 style='text-align: center; color: #FF5733;'>Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø±ÙŠÙ‚</h4>", unsafe_allow_html=True)

# âœ… Ø¥Ø¶Ø§ÙØ© Ø®ÙŠØ§Ø± Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
mode = st.sidebar.radio("ğŸ“Œ Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:", ["ğŸ¥ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©", "ğŸ“‚ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ"])

# âœ… 1ï¸âƒ£ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¹Ø¨Ø± `Streamlit WebRTC`
if mode == "ğŸ¥ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©":
    st.sidebar.warning("âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ù…ØªØµÙØ­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")

    class FireDetectionTransformer(VideoTransformerBase):
        def transform(self, frame):
            img = frame.to_ndarray(format="bgr24")

            # ğŸ”¹ ØªØ´ØºÙŠÙ„ YOLOv5 Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ
            results = model(img, conf=0.1)  # Ø¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø© Ø¥Ù„Ù‰ 0.1

            fire_detected = False
            for *xyxy, conf, cls in results.xyxy[0]:
                x1, y1, x2, y2 = map(int, xyxy)
                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)
                cv2.putText(img, "ğŸ”¥ Fire Detected", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                fire_detected = True

            if fire_detected:
                # ğŸ”´ Ø¥Ù†Ø°Ø§Ø± Ø¶ÙˆØ¦ÙŠ ÙˆÙ…ÙŠØ¶ Ø£Ø­Ù…Ø±
                for _ in range(5):
                    st.markdown("<div style='background-color: red; color: white; font-size: 24px; text-align: center;'>ğŸš¨ğŸ”¥ Ø¥Ù†Ø°Ø§Ø± Ø­Ø±ÙŠÙ‚! ğŸ”¥ğŸš¨</div>", unsafe_allow_html=True)
                    time.sleep(0.5)
                    st.markdown("<div style='background-color: white; color: white; font-size: 24px; text-align: center;'> </div>", unsafe_allow_html=True)
                    time.sleep(0.5)

                # ğŸ”Š ØªØ´ØºÙŠÙ„ Ø¥Ù†Ø°Ø§Ø± ØµÙˆØªÙŠ
                st.audio("mixkit-urgent-simple-tone-loop-2976.wav", autoplay=True)

            return av.VideoFrame.from_ndarray(img, format="bgr24")

    webrtc_streamer(key="fire-detection", video_transformer_factory=FireDetectionTransformer)

# âœ… 2ï¸âƒ£ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§
elif mode == "ğŸ“‚ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ":
    uploaded_file = st.sidebar.file_uploader("ğŸ“¸ Ù‚Ù… Ø¨Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ", type=["jpg", "png", "jpeg", "mp4"])

    if uploaded_file:
        file_type = uploaded_file.type.split("/")[0]

        if file_type == "image":
            # âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
            image = Image.open(uploaded_file)
            image_np = np.array(image)

            # ğŸ”¹ ØªØ´ØºÙŠÙ„ YOLOv5 Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
            results = model(image_np, conf=0.1)  # Ø¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø© Ø¥Ù„Ù‰ 0.1

            fire_detected = False
            for *xyxy, conf, cls in results.xyxy[0]:
                x1, y1, x2, y2 = map(int, xyxy)
                cv2.rectangle(image_np, (x1, y1), (x2, y2), (0, 0, 255), 2)
                fire_detected = True

            # âœ… Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            st.image(image_np, caption="ğŸ” Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©", use_column_width=True)

            if fire_detected:
                st.success("âœ… ğŸ”¥ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø­Ø±ÙŠÙ‚!")
                st.audio("mixkit-urgent-simple-tone-loop-2976.wav", autoplay=True)
            else:
                st.success("âœ… Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ø±ÙŠÙ‚ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©.")

        elif file_type == "video":
            # âœ… ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØªØ­Ù„ÙŠÙ„Ù‡ Ø¥Ø·Ø§Ø± Ø¨Ø¥Ø·Ø§Ø±
            video_path = "uploaded_video.mp4"
            with open(video_path, "wb") as f:
                f.write(uploaded_file.read())

            cap = cv2.VideoCapture(video_path)
            stframe = st.empty()

            fire_detected = False
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                # ğŸ”¹ ØªØ´ØºÙŠÙ„ YOLOv5 Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ
                results = model(frame, conf=0.1)  # Ø¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø© Ø¥Ù„Ù‰ 0.1

                for *xyxy, conf, cls in results.xyxy[0]:
                    x1, y1, x2, y2 = map(int, xyxy)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                    fire_detected = True

                # âœ… Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                stframe.image(frame_rgb, caption="ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ", use_column_width=True)

            cap.release()

            if fire_detected:
                st.success("âœ… ğŸ”¥ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø­Ø±ÙŠÙ‚!")
                st.audio("mixkit-urgent-simple-tone-loop-2976.wav", autoplay=True)
            else:
                st.success("âœ… Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ø±ÙŠÙ‚ ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ.")

import os
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import torch
import urllib.request
import cv2
import av
import numpy as np
from PIL import Image
from datetime import datetime
import pandas as pd
import time

# âœ… ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
os.system("pip install --upgrade ultralytics opencv-python-headless streamlit-webrtc")

# âœ… ØªØ­Ù…ÙŠÙ„ `best.pt` Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ Ø£Ùˆ ÙƒØ§Ù† ØªØ§Ù„ÙÙ‹Ø§
MODEL_PATH = "best.pt"
MODEL_URL = "https://raw.githubusercontent.com/msj78598/Fire-Detection-Monitoring-System/main/best.pt"

if not os.path.exists(MODEL_PATH) or os.path.getsize(MODEL_PATH) < 10000:
    print("âŒ Ù…Ù„Ù best.pt ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØªØ§Ù„ÙØŒ Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„Ù‡...")
    urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ best.pt Ø¨Ù†Ø¬Ø§Ø­!")

# âœ… ØªØ­Ù…ÙŠÙ„ YOLOv5 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `torch.hub.load()`
try:
    model = torch.hub.load(
        "ultralytics/yolov5",
        "custom",
        path=MODEL_PATH,
        source="github",
        force_reload=True
    )
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ YOLOv5 Ø¨Ù†Ø¬Ø§Ø­!")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ YOLOv5: {e}")

# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="Fire Detection Monitoring", page_icon="ğŸ”¥", layout="wide")

# âœ… Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
st.sidebar.title("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
st.sidebar.subheader("ğŸ“Š Ø¥ØµØ¯Ø§Ø± ØªÙ‚Ø±ÙŠØ±")
start_date = st.sidebar.date_input("ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©")
end_date = st.sidebar.date_input("ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©")

# âœ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
mode = st.sidebar.radio("ğŸ“Œ Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:", ["ğŸ¥ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©", "ğŸ“‚ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ"])

# âœ… 1ï¸âƒ£ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¹Ø¨Ø± `Streamlit WebRTC`
if mode == "ğŸ¥ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©":
    st.sidebar.warning("âš ï¸ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ù…ØªØµÙØ­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§.")

    class FireDetectionTransformer(VideoTransformerBase):
        def transform(self, frame):
            img = frame.to_ndarray(format="bgr24")
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # ğŸ”¹ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ RGB

            # ğŸ”¹ ØªØ´ØºÙŠÙ„ YOLOv5 Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ
            results = model(img_rgb, size=640, conf=0.3)  # âœ… Ø¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø© Ø¥Ù„Ù‰ 0.3

            fire_detected = False
            for *xyxy, conf, cls in results.xyxy[0]:
                if conf > 0.3:  # ğŸ”¥ Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø© 0.3
                    fire_detected = True
                    x1, y1, x2, y2 = map(int, xyxy)
                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)
                    cv2.putText(img, "ğŸ”¥ Fire Detected", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

            if fire_detected:
                st.warning("ğŸš¨ğŸ”¥ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø­Ø±ÙŠÙ‚! ğŸ”¥ğŸš¨")
                st.audio("mixkit-urgent-simple-tone-loop-2976.wav", autoplay=True)

            return av.VideoFrame.from_ndarray(img, format="bgr24")

    webrtc_streamer(key="fire-detection", video_transformer_factory=FireDetectionTransformer)

# âœ… 2ï¸âƒ£ ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ
elif mode == "ğŸ“‚ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ":
    uploaded_file = st.sidebar.file_uploader("ğŸ“¸ Ù‚Ù… Ø¨Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ", type=["jpg", "png", "jpeg", "mp4"])

    if uploaded_file:
        file_type = uploaded_file.type.split("/")[0]

        if file_type == "image":
            image = Image.open(uploaded_file)
            image_np = np.array(image)

            # ğŸ”¹ ØªØ´ØºÙŠÙ„ YOLOv5 Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
            results = model(image_np, size=640, conf=0.3)

            # ğŸ”¹ Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
            for *xyxy, conf, cls in results.xyxy[0]:
                if conf > 0.3:
                    x1, y1, x2, y2 = map(int, xyxy)
                    cv2.rectangle(image_np, (x1, y1), (x2, y2), (0, 0, 255), 2)

            st.image(image_np, caption="ğŸ” Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©", use_column_width=True)
            st.success("âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­!")

        elif file_type == "video":
            video_path = "uploaded_video.mp4"
            with open(video_path, "wb") as f:
                f.write(uploaded_file.read())

            cap = cv2.VideoCapture(video_path)
            stframe = st.empty()

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                results = model(frame, size=640, conf=0.3)

                for *xyxy, conf, cls in results.xyxy[0]:
                    if conf > 0.3:
                        x1, y1, x2, y2 = map(int, xyxy)
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)

                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                stframe.image(frame_rgb, caption="ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ", use_column_width=True)

            cap.release()
            st.success("âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø¬Ø§Ø­!")
